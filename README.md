# ML-algorithms-from-scratch-using-python
[<ins>Dataset</ins>](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/players_20.csv)
<br><br>
[<ins>Attributes of the dataset and their description</ins>](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/info.xlsx)
<br><br>
<b>Algorithms Implemented:</b><br><br>
(1) [Linear Regression ( simple + multiple ) ](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(1)%20Linear%20Regression.ipynb)
<br>
(2) [Decision Tree ID3](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(2)%20Decision%20Tree%20ID3.ipynb)
<br>
(3) [K-means clustering](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(3)%20K_means%20clustering.ipynb)
<br>
(4) [Logistic Regression](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(4)%20Logistic%20Regression.ipynb)
<br>
(5) [Na√Øve Bayes](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(5)%20Na%C3%AFve%20Bayes.ipynb)
<br>
(6) [Support Vector Machine (SVM) ](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(6)%20SVM.ipynb)
<br>
(7) [K-Nearest Neighbours (KNN)](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(7)%20KNN.ipynb)
<br>
(8) [Principal Component Analysis (PCA)](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/(8)%20PCA.ipynb)
<br><br>
<b>Other things that are also implemented:</b><br><br>
(1) [EDA ( Exploratory Data Analysis )](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/EDA.ipynb)
<br>
(2) [Normalization techniques](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/normalization_techniques.ipynb): Min-Max, Z-score, Decimal Scaling
<br>
(3) [Numeric similarity measures](https://github.com/riya-joshi-401/ML-algorithms-from-scratch-using-python/blob/main/numeric_similarity_measures.ipynb): Euclidean , Manhattan ,  Supremum , Mahalanobis
<br><br>
<b>TODO: </b>
<li> Random forest</li>
<li> Gradient boosting algorithms</li>
<li> Various other regression algorithms like: Ridge, Lasso</li>
<li> Artificial Neural network</li><br><br>
<b>NOTE: </b><br><br>
<li> This particular dataset was chosen as it contains various data types(string, integer, float) and very large number of records( approx 19,000 ) and attributes (72)</li>
<li> Short-comings of this dataset: Highly biased, unbalanced records.</li>
<br><br>

